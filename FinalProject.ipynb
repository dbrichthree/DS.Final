{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d06b5b4-bc41-48d6-933f-af9efb450ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import shutil\n",
    "import glob\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from delta import configure_spark_with_delta_pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3301da49-5a52-4a8d-8b2b-9b8ae922f975",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connections\n",
    "mysql_args = {\n",
    "    \"uid\": \"root\",\n",
    "    \"pwd\": \"ILike5Trains.\",\n",
    "    \"hostname\": \"localhost\",\n",
    "    \"dbname\": \"adventureworks\"\n",
    "}\n",
    "\n",
    "dw_name = \"aw_dw\"\n",
    "\n",
    "mongodb_args = {\n",
    "    \"user_name\": \"dbrichthree\",\n",
    "    \"password\": \"ILike5Trains.\",\n",
    "    \"cluster_name\": \"Cluster0\",\n",
    "    \"cluster_subnet\": \"pjne9qy\",\n",
    "    \"cluster_location\": \"atlas\",\n",
    "    \"db_name\": \"aw_mongo\",\n",
    "    \"collection\": \"\",\n",
    "    \"null_column_threshold\": 0.5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07cd11d5-0de3-41ce-a983-30b8e063b201",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Directorys\n",
    "dest_database = \"aw_final_dlh\"\n",
    "sql_warehouse_dir = os.path.abspath('spark-warehouse')\n",
    "database_dir = os.path.join(sql_warehouse_dir, dest_database)\n",
    "\n",
    "stream_root = os.path.join(os.getcwd(), \"stream_data\")\n",
    "sales_stream_dir = os.path.join(stream_root, \"sales_order_detail\")\n",
    "\n",
    "dim_dir = os.path.join(database_dir, \"dim\")\n",
    "fact_dir = os.path.join(database_dir, \"fact_sales_order_lines\")\n",
    "\n",
    "dim_date_path = os.path.join(dim_dir, \"dim_date\")\n",
    "dim_products_path = os.path.join(dim_dir, \"dim_products\")\n",
    "dim_suppliers_path = os.path.join(dim_dir, \"dim_suppliers\")\n",
    "\n",
    "fact_bronze_path = os.path.join(fact_dir, \"bronze\")\n",
    "fact_silver_path = os.path.join(fact_dir, \"silver\")\n",
    "fact_gold_path = os.path.join(fact_dir, \"gold\")\n",
    "\n",
    "chk_dir = os.path.join(database_dir, \"_checkpoints\")\n",
    "bronze_chk = os.path.join(chk_dir, \"fact_sales_bronze\")\n",
    "\n",
    "for p in [\n",
    "    database_dir, stream_root, sales_stream_dir,\n",
    "    dim_dir, fact_dir,\n",
    "    dim_date_path, dim_products_path, dim_suppliers_path,\n",
    "    fact_bronze_path, fact_silver_path, fact_gold_path,\n",
    "    chk_dir, bronze_chk\n",
    "]:\n",
    "    os.makedirs(p, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "085702b4-e779-4333-ba85-6ebf03d2cfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set ups\n",
    "def remove_directory_tree(path: str):\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            shutil.rmtree(path)\n",
    "        except PermissionError:\n",
    "            ts = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "            shutil.move(path, f\"{path}__OLD__{ts}\")\n",
    "\n",
    "def get_mongo_uri(**args):\n",
    "    if args[\"cluster_location\"] == \"atlas\":\n",
    "        uri = f\"mongodb+srv://{args['user_name']}:{args['password']}@\"\n",
    "        uri += f\"{args['cluster_name']}.{args['cluster_subnet']}.mongodb.net/\"\n",
    "    else:\n",
    "        uri = \"mongodb://localhost:27017/\"\n",
    "    return uri\n",
    "\n",
    "def get_spark_conf_args(spark_jars: list, **args):\n",
    "    jars = \"\"\n",
    "    for jar in spark_jars:\n",
    "        jars += f\"{jar}, \"\n",
    "    sparkConf_args = {\n",
    "        \"app_name\": \"PySpark AdventureWorks Final (Streaming + Delta)\",\n",
    "        \"worker_threads\": f\"local[{int(os.cpu_count()/2)}]\",\n",
    "        \"shuffle_partitions\": int(os.cpu_count()),\n",
    "        \"mongo_uri\": get_mongo_uri(**args),\n",
    "        \"spark_jars\": jars[0:-2],\n",
    "        \"database_dir\": sql_warehouse_dir\n",
    "    }\n",
    "    return sparkConf_args\n",
    "\n",
    "def get_jdbc_url(**args):\n",
    "    return f\"jdbc:mysql://{args['hostname']}:3306/{args['dbname']}\"\n",
    "\n",
    "def get_mysql_spark_df(spark_session, sql_query: str, **args):\n",
    "    return (\n",
    "        spark_session.read.format(\"jdbc\")\n",
    "        .option(\"url\", get_jdbc_url(**args))\n",
    "        .option(\"driver\", \"com.mysql.cj.jdbc.Driver\")\n",
    "        .option(\"user\", args[\"uid\"])\n",
    "        .option(\"password\", args[\"pwd\"])\n",
    "        .option(\"query\", sql_query)\n",
    "        .load()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10a158be-187b-4167-9559-c9bb3cd7e049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a Spark Session\n",
    "remove_directory_tree(database_dir)\n",
    "os.makedirs(database_dir, exist_ok=True)\n",
    "\n",
    "jars = []\n",
    "\n",
    "SPARK_HOME = os.environ.get(\"SPARK_HOME\", r\"C:\\spark-3.5.4-bin-hadoop3\")\n",
    "mysql_candidates = glob.glob(os.path.join(SPARK_HOME, \"jars\", \"mysql-connector-j-*.jar\")) + \\\n",
    "                   glob.glob(os.path.join(SPARK_HOME, \"jars\", \"mysql-connector-java-*.jar\"))\n",
    "\n",
    "if len(mysql_candidates) == 0:\n",
    "    raise RuntimeError(f\"No MySQL JDBC jar found in {os.path.join(SPARK_HOME, 'jars')}. Put mysql-connector-j-8.0.33.jar there.\")\n",
    "\n",
    "jars.append(mysql_candidates[0])\n",
    "\n",
    "def get_spark_conf(**args):\n",
    "    sparkConf = SparkConf().setAppName(args['app_name']) \\\n",
    "        .setMaster(args['worker_threads']) \\\n",
    "        .set('spark.driver.memory', '4g') \\\n",
    "        .set('spark.executor.memory', '2g') \\\n",
    "        .set('spark.jars', args['spark_jars']) \\\n",
    "        .set('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:3.0.1') \\\n",
    "        .set('spark.mongodb.input.uri', args['mongo_uri']) \\\n",
    "        .set('spark.mongodb.output.uri', args['mongo_uri']) \\\n",
    "        .set('spark.sql.adaptive.enabled', 'false') \\\n",
    "        .set('spark.sql.debug.maxToStringFields', 50) \\\n",
    "        .set('spark.sql.shuffle.partitions', args['shuffle_partitions']) \\\n",
    "        .set('spark.sql.streaming.forceDeleteTempCheckpointLocation', 'true') \\\n",
    "        .set('spark.sql.streaming.schemaInference', 'true') \\\n",
    "        .set('spark.sql.warehouse.dir', database_dir) \\\n",
    "        .set('spark.streaming.stopGracefullyOnShutdown', 'true') \\\n",
    "        .set(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n",
    "        .set(\"spark.driver.host\", \"127.0.0.1\") \\\n",
    "        .set(\"spark.ui.enabled\", \"false\") \\\n",
    "        .set(\"spark.port.maxRetries\", \"200\")\n",
    "    return sparkConf\n",
    "\n",
    "sparkConf_args = get_spark_conf_args(jars, **mongodb_args)\n",
    "sparkConf = get_spark_conf(**sparkConf_args)\n",
    "\n",
    "builder = SparkSession.builder.config(conf=sparkConf) \\\n",
    "    .config('spark.sql.extensions', 'io.delta.sql.DeltaSparkSessionExtension') \\\n",
    "    .config('spark.sql.catalog.spark_catalog', 'org.apache.spark.sql.delta.catalog.DeltaCatalog')\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"OFF\")\n",
    "\n",
    "spark.sql(f\"DROP DATABASE IF EXISTS {dest_database} CASCADE;\")\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {dest_database};\")\n",
    "spark.sql(f\"USE {dest_database};\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08ff05a2-6eea-4b52-97de-86202edc6b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting SQL Data\n",
    "sql_products = \"SELECT ProductID AS product_id, Name AS product_name FROM product\"\n",
    "df_products_src = get_mysql_spark_df(spark, sql_products, **mysql_args)\n",
    "\n",
    "sql_vendor = \"SELECT VendorID AS supplier_id, Name AS company FROM vendor\"\n",
    "df_suppliers_src = get_mysql_spark_df(spark, sql_vendor, **mysql_args)\n",
    "\n",
    "sql_sales = \"\"\"\n",
    "SELECT sod.SalesOrderDetailID AS order_detail_id,\n",
    "       soh.SalesOrderID AS order_id,\n",
    "       sod.ProductID AS product_id,\n",
    "       soh.OrderDate AS order_date,\n",
    "       sod.OrderQty AS quantity,\n",
    "       sod.UnitPrice AS unit_price\n",
    "FROM salesorderheader AS soh\n",
    "JOIN salesorderdetail AS sod\n",
    "  ON soh.SalesOrderID = sod.SalesOrderID\n",
    "\"\"\"\n",
    "df_sales_src = get_mysql_spark_df(spark, sql_sales, **mysql_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eff49eb7-1b60-4486-aaad-1c2be0284e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensions\n",
    "df_dates = (\n",
    "    df_sales_src\n",
    "    .select(to_date(col(\"order_date\")).alias(\"full_date\"))\n",
    "    .dropna()\n",
    "    .dropDuplicates([\"full_date\"])\n",
    "    .withColumn(\"date_key\", date_format(col(\"full_date\"), \"yyyyMMdd\").cast(\"int\"))\n",
    "    .withColumn(\"year\", year(col(\"full_date\")))\n",
    "    .withColumn(\"month\", month(col(\"full_date\")))\n",
    "    .withColumn(\"day\", dayofmonth(col(\"full_date\")))\n",
    "    .select(\"date_key\", \"full_date\", \"year\", \"month\", \"day\")\n",
    ")\n",
    "\n",
    "df_products = (\n",
    "    df_products_src\n",
    "    .dropna()\n",
    "    .dropDuplicates([\"product_id\"])\n",
    "    .withColumn(\"product_key\", row_number().over(Window.orderBy(col(\"product_id\"))).cast(\"int\"))\n",
    "    .select(\"product_key\", \"product_id\", \"product_name\")\n",
    ")\n",
    "\n",
    "df_suppliers = (\n",
    "    df_suppliers_src\n",
    "    .dropna()\n",
    "    .dropDuplicates([\"supplier_id\"])\n",
    "    .withColumn(\"supplier_key\", row_number().over(Window.orderBy(col(\"supplier_id\"))).cast(\"int\"))\n",
    "    .select(\"supplier_key\", \"supplier_id\", \"company\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08ca6c10-69a5-47ec-bd9d-0238105f665b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Dimensions\n",
    "(df_dates.write.format(\"delta\").mode(\"overwrite\").save(dim_date_path))\n",
    "(df_products.write.format(\"delta\").mode(\"overwrite\").save(dim_products_path))\n",
    "(df_suppliers.write.format(\"delta\").mode(\"overwrite\").save(dim_suppliers_path))\n",
    "\n",
    "dim_date_loc = dim_date_path.replace(\"\\\\\", \"/\")\n",
    "dim_products_loc = dim_products_path.replace(\"\\\\\", \"/\")\n",
    "dim_suppliers_loc = dim_suppliers_path.replace(\"\\\\\", \"/\")\n",
    "\n",
    "spark.sql(f\"CREATE TABLE IF NOT EXISTS dim_date USING DELTA LOCATION '{dim_date_loc}'\")\n",
    "spark.sql(f\"CREATE TABLE IF NOT EXISTS dim_products USING DELTA LOCATION '{dim_products_loc}'\")\n",
    "spark.sql(f\"CREATE TABLE IF NOT EXISTS dim_suppliers USING DELTA LOCATION '{dim_suppliers_loc}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69fd2618-7d9d-4204-aa4c-00ef52084a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- STREAM (FACT -> BRONZE: DELTA) ----------\n",
    "sales_schema = StructType([\n",
    "    StructField(\"SalesOrderDetailID\", IntegerType(), True),\n",
    "    StructField(\"SalesOrderID\", IntegerType(), True),\n",
    "    StructField(\"ProductID\", IntegerType(), True),\n",
    "    StructField(\"OrderDate\", StringType(), True),\n",
    "    StructField(\"OrderQty\", IntegerType(), True),\n",
    "    StructField(\"UnitPrice\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "df_sales_stream = (\n",
    "    spark.readStream\n",
    "    .schema(sales_schema)\n",
    "    .json(sales_stream_dir)\n",
    "    .selectExpr(\n",
    "        \"SalesOrderDetailID as order_detail_id\",\n",
    "        \"SalesOrderID as order_id\",\n",
    "        \"ProductID as product_id\",\n",
    "        \"to_date(OrderDate) as order_date\",\n",
    "        \"OrderQty as quantity\",\n",
    "        \"UnitPrice as unit_price\"\n",
    "    )\n",
    ")\n",
    "\n",
    "bronze_query = (\n",
    "    df_sales_stream\n",
    "    .writeStream\n",
    "    .format(\"delta\")\n",
    "    .option(\"checkpointLocation\", bronze_chk)\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .trigger(availableNow=True)\n",
    "    .start(fact_bronze_path)\n",
    ")\n",
    "\n",
    "bronze_query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a73d7f83-6171-420d-b7fb-052e986824c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dim Products\n",
    "sql_products = \"\"\"\n",
    "SELECT ProductID AS product_id, Name AS product_name\n",
    "FROM product\n",
    "\"\"\"\n",
    "\n",
    "df_products_src = get_mysql_spark_df(spark, sql_products, **mysql_args)\n",
    "\n",
    "df_products = (\n",
    "    df_products_src\n",
    "    .dropna()\n",
    "    .dropDuplicates([\"product_id\"])\n",
    "    .withColumn(\n",
    "        \"product_key\",\n",
    "        row_number().over(Window.orderBy(col(\"product_id\"))).cast(\"int\")\n",
    "    )\n",
    "    .select(\"product_key\", \"product_id\", \"product_name\")\n",
    ")\n",
    "\n",
    "df_products.write.format(\"delta\").mode(\"overwrite\").save(dim_products_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "afeff0a1-f3d1-4354-a64e-0c62a85cb55b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---------- SILVER (FACT + DIM KEYS) ----------\n",
    "df_bronze = spark.read.format(\"delta\").load(fact_bronze_path)\n",
    "df_dim_products = spark.read.format(\"delta\").load(dim_products_path)\n",
    "\n",
    "df_silver = (\n",
    "    df_bronze\n",
    "    .withColumn(\"order_date_key\", date_format(col(\"order_date\"), \"yyyyMMdd\").cast(\"int\"))\n",
    "    .join(df_dim_products.select(\"product_id\", \"product_key\"), on=\"product_id\", how=\"inner\")\n",
    "    .select(\n",
    "        col(\"order_detail_id\"),\n",
    "        col(\"order_id\"),\n",
    "        col(\"product_key\"),\n",
    "        col(\"order_date_key\"),\n",
    "        col(\"quantity\"),\n",
    "        col(\"unit_price\")\n",
    "    )\n",
    "    .withColumn(\"fact_sales_key\", row_number().over(Window.orderBy(col(\"order_detail_id\"))).cast(\"int\"))\n",
    "    .select(\"fact_sales_key\", \"order_detail_id\", \"order_id\", \"product_key\", \"order_date_key\", \"quantity\", \"unit_price\")\n",
    ")\n",
    "\n",
    "df_silver.write.format(\"delta\").mode(\"overwrite\").save(fact_silver_path)\n",
    "\n",
    "fact_silver_loc = fact_silver_path.replace(\"\\\\\", \"/\")\n",
    "spark.sql(f\"CREATE TABLE IF NOT EXISTS fact_sales_silver USING DELTA LOCATION '{fact_silver_loc}'\")\n",
    "\n",
    "print(spark.read.format(\"delta\").load(fact_silver_path).count())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3055e906-98e4-487c-adde-a5d36636d7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dim Date\n",
    "sql_dates = \"\"\"\n",
    "SELECT DISTINCT DATE(soh.OrderDate) AS full_date\n",
    "FROM salesorderheader soh\n",
    "\"\"\"\n",
    "df_dates_src = get_mysql_spark_df(spark, sql_dates, **mysql_args)\n",
    "\n",
    "df_dates = (\n",
    "    df_dates_src\n",
    "    .withColumn(\"full_date\", to_date(col(\"full_date\")))\n",
    "    .withColumn(\"date_key\", date_format(col(\"full_date\"), \"yyyyMMdd\").cast(\"int\"))\n",
    "    .withColumn(\"year\", year(col(\"full_date\")))\n",
    "    .withColumn(\"month\", month(col(\"full_date\")))\n",
    "    .withColumn(\"day\", dayofmonth(col(\"full_date\")))\n",
    "    .select(\"date_key\", \"full_date\", \"year\", \"month\", \"day\")\n",
    ")\n",
    "\n",
    "df_dates.write.format(\"delta\").mode(\"overwrite\").save(dim_date_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8fa177d1-7cae-4d33-84c1-bf57f5e374e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---------- GOLD (AGGREGATES) ----------\n",
    "df_dim_date = spark.read.format(\"delta\").load(dim_date_path)\n",
    "df_dim_products = spark.read.format(\"delta\").load(dim_products_path)\n",
    "df_silver = spark.read.format(\"delta\").load(fact_silver_path)\n",
    "\n",
    "df_gold = (\n",
    "    df_silver.alias(\"f\")\n",
    "    .join(\n",
    "        df_dim_date.select(col(\"date_key\").alias(\"order_date_key\"), \"year\", \"month\").alias(\"d\"),\n",
    "        on=\"order_date_key\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    .join(\n",
    "        df_dim_products.select(\"product_key\", \"product_name\").alias(\"p\"),\n",
    "        on=\"product_key\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    .groupBy(\"year\", \"month\", \"product_name\")\n",
    "    .agg(\n",
    "        sum(\"quantity\").alias(\"total_qty\"),\n",
    "        round(sum(col(\"quantity\") * col(\"unit_price\")), 2).alias(\"revenue_est\")\n",
    "    )\n",
    "    .orderBy(col(\"revenue_est\").desc())\n",
    ")\n",
    "\n",
    "df_gold.write.format(\"delta\").mode(\"overwrite\").save(fact_gold_path)\n",
    "\n",
    "fact_gold_loc = fact_gold_path.replace(\"\\\\\", \"/\")\n",
    "spark.sql(f\"CREATE TABLE IF NOT EXISTS fact_sales_gold USING DELTA LOCATION '{fact_gold_loc}'\")\n",
    "\n",
    "print(spark.read.format(\"delta\").load(fact_gold_path).count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2fd2501b-add7-46be-baae-ca826e6b5d65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|bronze_rows|\n",
      "+-----------+\n",
      "|          2|\n",
      "+-----------+\n",
      "\n",
      "+-----------+\n",
      "|silver_rows|\n",
      "+-----------+\n",
      "|          2|\n",
      "+-----------+\n",
      "\n",
      "+---------+\n",
      "|gold_rows|\n",
      "+---------+\n",
      "|        2|\n",
      "+---------+\n",
      "\n",
      "+----+-----+------------+---------+-----------+\n",
      "|year|month|product_name|total_qty|revenue_est|\n",
      "+----+-----+------------+---------+-----------+\n",
      "+----+-----+------------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ---------- ORIGINAL VALIDATION QUERIES ----------\n",
    "fact_bronze_loc = fact_bronze_path.replace(\"\\\\\", \"/\")\n",
    "spark.sql(f\"CREATE TABLE IF NOT EXISTS fact_sales_bronze USING DELTA LOCATION '{fact_bronze_loc}'\")\n",
    "\n",
    "fact_silver_loc = fact_silver_path.replace(\"\\\\\", \"/\")\n",
    "spark.sql(f\"CREATE TABLE IF NOT EXISTS fact_sales_silver USING DELTA LOCATION '{fact_silver_loc}'\")\n",
    "\n",
    "dim_products_loc = dim_products_path.replace(\"\\\\\", \"/\")\n",
    "spark.sql(f\"CREATE TABLE IF NOT EXISTS dim_products USING DELTA LOCATION '{dim_products_loc}'\")\n",
    "\n",
    "dim_date_loc = dim_date_path.replace(\"\\\\\", \"/\")\n",
    "spark.sql(f\"CREATE TABLE IF NOT EXISTS dim_date USING DELTA LOCATION '{dim_date_loc}'\")\n",
    "\n",
    "spark.sql(\"SELECT COUNT(*) AS bronze_rows FROM fact_sales_bronze\").show()\n",
    "spark.sql(\"SELECT COUNT(*) AS silver_rows FROM fact_sales_silver\").show()\n",
    "spark.sql(\"SELECT COUNT(*) AS gold_rows FROM fact_sales_gold\").show()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "  d.year,\n",
    "  d.month,\n",
    "  p.product_name,\n",
    "  SUM(f.quantity) AS total_qty,\n",
    "  ROUND(SUM(f.quantity * f.unit_price), 2) AS revenue_est\n",
    "FROM fact_sales_silver f\n",
    "JOIN dim_date d     ON f.order_date_key = d.date_key\n",
    "JOIN dim_products p ON f.product_key = p.product_key\n",
    "GROUP BY d.year, d.month, p.product_name\n",
    "ORDER BY revenue_est DESC\n",
    "LIMIT 25\n",
    "\"\"\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dd5b3a-d8eb-410d-bea4-3db5973c39be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ds2002-env)",
   "language": "python",
   "name": "ds2002-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
